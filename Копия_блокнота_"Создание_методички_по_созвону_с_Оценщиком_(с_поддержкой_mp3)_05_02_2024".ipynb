{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KristinaFateyeva/test/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22%D0%A1%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D0%B8%D1%87%D0%BA%D0%B8_%D0%BF%D0%BE_%D1%81%D0%BE%D0%B7%D0%B2%D0%BE%D0%BD%D1%83_%D1%81_%D0%9E%D1%86%D0%B5%D0%BD%D1%89%D0%B8%D0%BA%D0%BE%D0%BC_(%D1%81_%D0%BF%D0%BE%D0%B4%D0%B4%D0%B5%D1%80%D0%B6%D0%BA%D0%BE%D0%B9_mp3)_05_02_2024%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Настройка окружения\n",
        "\n",
        "!pip install faiss-cpu langchain openai tiktoken moviepy pydub nltk >/dev/null 2>&1\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import NLTKTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from IPython.display import display\n",
        "from IPython.display import HTML, clear_output\n",
        "import tiktoken\n",
        "import zipfile\n",
        "import requests\n",
        "from moviepy.editor import VideoFileClip\n",
        "from pydub import AudioSegment\n",
        "import nltk\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "from tqdm.notebook import trange, tqdm\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "print('Настройка завершилась удачно!')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ds_vcvGfbuwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Установка ключа OpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Введите OpenAI API Key и нажмите Enter:\")\n",
        "client = OpenAI(\n",
        "  api_key=os.environ['OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
        ")\n",
        "\n",
        "# Функция настройки стиля для переноса текста в выводе ячеек\n",
        "def set_text_wrap_css():\n",
        "    css = '''\n",
        "    <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "    </style>\n",
        "    '''\n",
        "    display(HTML(css))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', set_text_wrap_css)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MZHE2EefsPb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Загрузка и обработка аудио (.mp3, .m4a) по ссылке Яндекс диска\n",
        "url_file = \"\" #@param {type:\"string\"}\n",
        "\n",
        "def get_source_file(path_base):\n",
        "  source_path = ''\n",
        "  type_file = ''\n",
        "  audio_path = ''\n",
        "  full_url = f'https://getfile.dokpub.com/yandex/get/{path_base}'\n",
        "  response = requests.get(full_url)\n",
        "  extract_dir = '/content'\n",
        "  headers_file = response.headers['Content-Disposition']\n",
        "\n",
        "  if headers_file.endswith('.mp4'):\n",
        "    type_file = 'mp4'\n",
        "    source_path = '/content/source_load_file.mp4'\n",
        "    with open(source_path, 'wb') as file:\n",
        "      file.write(response.content)\n",
        "    print(f'Загружен файл: {headers_file[-19:]}')\n",
        "\n",
        "  elif headers_file.endswith('.m4a'):\n",
        "    type_file = 'm4a'\n",
        "    source_path = '/content/source_load_file.m4a'\n",
        "    with open(source_path, 'wb') as file:\n",
        "      file.write(response.content)\n",
        "    print(f'Загружен файл: {headers_file[-19:]}')\n",
        "\n",
        "  elif headers_file.endswith('.mp3'):\n",
        "    type_file = 'mp3'\n",
        "    source_path = '/content/source_load_file.mp3'\n",
        "    with open(source_path, 'wb') as file:\n",
        "      file.write(response.content)\n",
        "    print(f'Загружен файл: {headers_file[-19:]}')\n",
        "\n",
        "  else:\n",
        "    print(f'Не поддерживаемый тип файла (поддерживается только .m4a, .mp4 или .mp3): {headers_file.split[\".\"][-1]}')\n",
        "\n",
        "  return source_path, type_file\n",
        "\n",
        "\n",
        "def extract_audio(video_path, audio_path):\n",
        "    video_clip = VideoFileClip(video_path)\n",
        "    audio_clip = video_clip.audio\n",
        "    audio_clip.write_audiofile(audio_path)\n",
        "\n",
        "def convert_m4a_to_mp3(m4a_path, mp3_path):\n",
        "    audio = AudioSegment.from_file(m4a_path)\n",
        "    audio.export(mp3_path, format=\"mp3\")\n",
        "\n",
        "try:\n",
        "  source_file_path, type_file = get_source_file(url_file)\n",
        "\n",
        "  if type_file == 'mp4' and source_file_path is not None:\n",
        "      audio_path = 'source_load_file.mp3'\n",
        "      print('Конвертация видео...')\n",
        "      extract_audio(source_file_path, audio_path)\n",
        "      print('Конвертация видео ОК!')\n",
        "\n",
        "  elif type_file == 'm4a' and source_file_path is not None:\n",
        "      audio_path = 'source_load_file.mp3'\n",
        "      print('Конвертация аудио...')\n",
        "      convert_m4a_to_mp3(source_file_path, audio_path)\n",
        "      print('Конвертация аудио ОК!')\n",
        "\n",
        "  elif type_file == 'mp3' and source_file_path is not None:\n",
        "      audio_path = 'source_load_file.mp3'\n",
        "      print('Файл уже в формате mp3.')\n",
        "\n",
        "  else:\n",
        "      print(f'Проверьте правильность ссылки или типа файла и попробуйте снова!!!')\n",
        "\n",
        "  client = OpenAI()\n",
        "\n",
        "  audio_file= open(audio_path, \"rb\")\n",
        "  print('Транскрибация аудио...')\n",
        "  transcript = client.audio.transcriptions.create(\n",
        "    model=\"whisper-1\",\n",
        "    file=audio_file,\n",
        "    #prompt='На записи диалог 2 пользователей (клиент и менеджер) на видеоконференции. Соблюдай знаки припинания, заглавные буквы начала предложений.'\n",
        "  )\n",
        "  print('Транскрибация аудио ОК!')\n",
        "\n",
        "except Exception as e:\n",
        "  print(\"Ошибка: \", e)\n",
        "\n",
        "print(f\"Транскрибированный текст: \\n\\n {transcript.dict()['text']}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "L9itTenOLqJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Обработка текста\n",
        "verbose = False #@param {type: \"boolean\"}\n",
        "model = 'gpt-3.5-turbo-16k-0613' #@param ['gpt-3.5-turbo-16k-0613', \"gpt-3.5-turbo-1106\", \"gpt-4-1106-preview\"]\n",
        "system = '\\u041A\\u0442\\u043E \\u0442\\u044B \\u0438 \\u043A\\u0430\\u043A \\u0442\\u044B \\u0432\\u044B\\u043F\\u043E\\u043B\\u043D\\u044F\\u0435\\u0448\\u044C \\u0441\\u0432\\u043E\\u044E \\u0440\\u0430\\u0431\\u043E\\u0442\\u0443: \\u0412\\u044B \\u0443\\u043C\\u0435\\u0435\\u0442\\u0435 \\u043B\\u0443\\u0447\\u0448\\u0435 \\u0432\\u0441\\u0435\\u0445 \\u0432 \\u043C\\u0438\\u0440\\u0435 \\u0430\\u043D\\u0430\\u043B\\u0438\\u0437\\u0438\\u0440\\u043E\\u0432\\u0430\\u0442\\u044C \\u0438\\u043D\\u0444\\u043E\\u0440\\u043C\\u0430\\u0446\\u0438\\u044E. \\u041A\\u0430\\u043A\\u0438\\u0435 \\u0442\\u044B \\u0434\\u0430\\u043D\\u043D\\u044B\\u0435 \\u0442\\u044B \\u043E\\u0431\\u0440\\u0430\\u0431\\u0430\\u0442\\u044B\\u0432\\u0430\\u0435\\u0448\\u044C: \\u041F\\u0435\\u0440\\u0435\\u0434 \\u0442\\u043E\\u0431\\u043E\\u0439 \\u0442\\u0435\\u043A\\u0441\\u0442 \\u043E\\u0431\\u0449\\u0435\\u043D\\u0438\\u044F  \\u0437\\u0430\\u043A\\u0430\\u0437\\u0447\\u0438\\u043A\\u0430 \\u0438 \\u043C\\u0435\\u043D\\u0435\\u0434\\u0436\\u0435\\u0440\\u0430 \\u043F\\u043E \\u043E\\u0446\\u0435\\u043D\\u043A\\u0435 \\u0441\\u0442\\u043E\\u0438\\u043C\\u043E\\u0441\\u0442\\u0438 IT \\u043F\\u0440\\u043E\\u0435\\u043A\\u0442\\u0430.  \\u041A\\u0430\\u043A\\u0438\\u0435 \\u0443\\u0441\\u043B\\u043E\\u0432\\u0438\\u044F \\u0432\\u044B\\u043F\\u043E\\u043B\\u043D\\u0435\\u043D\\u0438\\u044F \\u0440\\u0430\\u0431\\u043E\\u0442\\u044B \\u0442\\u044B \\u0432\\u044B\\u043F\\u043E\\u043B\\u043D\\u044F\\u0435\\u0448\\u044C: \\u0422\\u044B \\u043E\\u043F\\u0438\\u0441\\u044B\\u0432\\u0430\\u0435\\u0448\\u044C \\u0442\\u0435\\u043A\\u0441\\u0442 \\u0442\\u0430\\u043A\\u0438\\u043C \\u043E\\u0431\\u0440\\u0430\\u0437\\u043E\\u043C, \\u0447\\u0442\\u043E\\u0431\\u044B \\u0431\\u044B\\u043B\\u043E \\u043F\\u043E\\u043D\\u044F\\u0442\\u043D\\u043E \\u043E \\u043A\\u0430\\u043A\\u0438\\u0445 \\u0442\\u0435\\u0445\\u043D\\u0438\\u0447\\u0435\\u0441\\u043A\\u0438\\u0445 \\u0434\\u0430\\u043D\\u043D\\u044B\\u0445 \\u0448\\u043B\\u0430 \\u0440\\u0435\\u0447\\u044C \\u0438 \\u043E \\u0447\\u0435\\u043C \\u0431\\u044B\\u043B  \\u0434\\u0438\\u0430\\u043B\\u043E\\u0433, \\u0442\\u044B \\u0432\\u0441\\u0435\\u0433\\u0434\\u0430 \\u0432\\u044B\\u043F\\u0438\\u0441\\u044B\\u0432\\u0430\\u0435\\u0448\\u044C \\u0442\\u043E\\u0447\\u043D\\u044B\\u0435 \\u0442\\u0435\\u0445\\u043D\\u0438\\u0447\\u0435\\u0441\\u043A\\u0438\\u0435 \\u0434\\u0430\\u043D\\u043D\\u044B\\u0435, \\u043E \\u043A\\u043E\\u0442\\u043E\\u0440\\u044B\\u0445 \\u0431\\u044B\\u043B\\u043E \\u043D\\u0430\\u043F\\u0438\\u0441\\u0430\\u043D\\u043E \\u0432 \\u0438\\u0441\\u0445\\u043E\\u0434\\u043D\\u043E\\u043C \\u0442\\u0435\\u043A\\u0441\\u0442\\u0435.' #@param {type:\"string\"}\n",
        "user = \"\\u041F\\u043E\\u0436\\u0430\\u043B\\u0443\\u0439\\u0441\\u0442\\u0430, \\u0434\\u0430\\u0432\\u0430\\u0439\\u0442\\u0435 \\u043F\\u043E\\u0434\\u0443\\u043C\\u0430\\u0435\\u043C \\u0448\\u0430\\u0433 \\u0437\\u0430 \\u0448\\u0430\\u0433\\u043E\\u043C: \\u043F\\u043E\\u0434\\u0443\\u043C\\u0430\\u0439 \\u043D\\u0430\\u0434 \\u0442\\u0435\\u043A\\u0441\\u0442\\u043E\\u043C, \\u043D\\u0430\\u043F\\u0438\\u0448\\u0438 \\u043E\\u0447\\u0435\\u043D\\u044C \\u043A\\u0440\\u0430\\u0442\\u043A\\u043E \\u0441\\u0443\\u0445\\u043E\\u0439 \\u0442\\u0435\\u0445\\u043D\\u0438\\u0447\\u0435\\u0441\\u043A\\u0438\\u0439 \\u043F\\u0435\\u0440\\u0435\\u0441\\u043A\\u0430\\u0437 \\u043E\\u0431\\u0449\\u0435\\u043D\\u0438\\u044F \\u0441\\u043F\\u0438\\u0441\\u043A\\u043E\\u043C \\u043F\\u043E  \\u0445\\u043E\\u0434\\u0443 \\u0434\\u0438\\u0430\\u043B\\u043E\\u0433\\u0430; \\u0412\\u0430\\u0436\\u043D\\u044B\\u0435 \\u0442\\u0435\\u0445\\u043D\\u0438\\u0447\\u0435\\u0441\\u043A\\u0438\\u0435 \\u0434\\u0430\\u043D\\u043D\\u044B\\u0435 \\u0434\\u043B\\u044F \\u043E\\u0446\\u0435\\u043D\\u043A\\u0438 \\u043F\\u0440\\u043E\\u0435\\u043A\\u0442\\u0430: \\\"\\u0421\\u043F\\u0438\\u0441\\u043E\\u043A \\u0412\\u0421\\u0415\\u0425 \\u0432\\u0430\\u0436\\u043D\\u044B\\u0445 \\u0442\\u0435\\u0445\\u043D\\u0438\\u0447\\u0435\\u0441\\u043A\\u0438\\u0445 \\u0434\\u0430\\u043D\\u043D\\u044B\\u0445 \\u043E\\u0431\\u0441\\u0443\\u0436\\u0434\\u0430\\u0435\\u043C\\u044B\\u0445 \\u0437\\u0430\\u043A\\u0430\\u0437\\u0447\\u0438\\u043A\\u043E\\u043C \\u043A\\u043E\\u0442\\u043E\\u0440\\u044B\\u0435 \\u0442\\u044B \\u0440\\u0430\\u0441\\u043F\\u043E\\u0437\\u043D\\u0430\\u043B \\u0432 \\u0438\\u0441\\u0445\\u043E\\u0434\\u043D\\u043E\\u043C \\u0442\\u0435\\u043A\\u0441\\u0442\\u0435\\\".\" #@param {type:\"string\"}\n",
        "temperature = 0 #@param {type: \"slider\", min: 0, max: 1, step:0.1}\n",
        "chunk_size_copywriter = 8000 # @param {type:\"slider\", min:1000, max:8000, step:100}\n",
        "fragment_size = 8000 # @param {type:\"slider\", min:4000, max:8000, step:100}\n",
        "result_file_path = 'result_copywriter.txt' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-16k-0613\"):\n",
        "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model in {\n",
        "        \"gpt-3.5-turbo-0613\",\n",
        "        \"gpt-3.5-turbo-16k-0613\",\n",
        "        \"gpt-3.5-turbo-16k\",\n",
        "        \"gpt-4-0314\",\n",
        "        \"gpt-4-32k-0314\",\n",
        "        \"gpt-4-0613\",\n",
        "        \"gpt-4-32k-0613\",\n",
        "        \"gpt-3.5-turbo-1106\",\n",
        "    }:\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    elif model == \"gpt-3.5-turbo-0301\":\n",
        "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
        "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
        "    elif \"gpt-3.5-turbo\" in model:\n",
        "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
        "    elif \"gpt-4\" in model:\n",
        "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
        "        )\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "    return num_tokens\n",
        "\n",
        "\n",
        "# Функция дробления текста на чанки\n",
        "def split_text(text, chunk_size=chunk_size_copywriter, verbose=verbose):\n",
        "    source_chunks = []\n",
        "    splitter = RecursiveCharacterTextSplitter(separators=['\\n', '\\n\\n', '. '], chunk_size=chunk_size, chunk_overlap=0)\n",
        "\n",
        "    for chunk in splitter.split_text(text):\n",
        "        source_chunks.append(Document(page_content=chunk, metadata={}))\n",
        "\n",
        "    if verbose:\n",
        "      print(f'\\n\\nФрагмент разбит на {len(source_chunks)} чанк[-а/-ов]:')\n",
        "\n",
        "    return source_chunks\n",
        "\n",
        "# Функция разбиения текста после парсинга на фрагменты\n",
        "def split_parced_text(text, fragment_size=fragment_size):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    result = []\n",
        "    current_segment = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_segment) + len(sentence) <= fragment_size:\n",
        "            current_segment += sentence + \" \"\n",
        "        else:\n",
        "            result.append(current_segment)\n",
        "            current_segment = sentence + \" \"\n",
        "\n",
        "    if current_segment:\n",
        "        result.append(current_segment)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Функция получения ответа от копирайтера\n",
        "def answer_index_copywriter(model, system, user, chunk, temperature=temperature):\n",
        "    client = OpenAI()\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': system},\n",
        "        {'role': 'user', 'content': user + f'\\n{chunk}'}\n",
        "    ]\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    if verbose:\n",
        "      print(f'\\n====================\\n\\n{num_tokens_from_messages(messages)} токенов будет использовано на чанк.\\n\\n')\n",
        "    answer = completion.choices[0].message.content\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Функция обработки одного фрагмента текста\n",
        "def process_one_fragment(fragment, model=model, system=system, user=user, verbose=verbose):\n",
        "    source_chunks = split_text(fragment)\n",
        "    processed_text = ''\n",
        "\n",
        "    for chunk in source_chunks:\n",
        "        attempt = 0\n",
        "\n",
        "        while attempt < 3:\n",
        "            try:\n",
        "                answer = answer_index_copywriter(model, system, user, chunk.page_content)\n",
        "                break\n",
        "\n",
        "            except Exception as e:\n",
        "                attempt += 1\n",
        "                print(f'\\n\\nПопытка {attempt} не удалась из-за ошибки: {str(e)}')\n",
        "                time.sleep(10)\n",
        "                if attempt == 3:\n",
        "                    answer = ''\n",
        "                    print(f'\\n\\nОбработка элемента не удалась после 3 попыток:\\n{chunk}')\n",
        "\n",
        "        processed_text += f'{answer}\\n\\n'\n",
        "        if verbose:\n",
        "          print (f'{answer}')\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "# Функция обработки всего текста копирайтером\n",
        "def process_data(text,\n",
        "                 fragment_size=fragment_size,\n",
        "                 system=system,\n",
        "                 user=user,\n",
        "                 result_file_path=result_file_path,\n",
        "                 verbose=verbose):\n",
        "\n",
        "    # Добавляем заголовок лендинга в результирующий текст (заголовок первого уровня)\n",
        "    result_text = ''\n",
        "\n",
        "    # Делим текст на фрагменты\n",
        "    fragments = split_parced_text(text, fragment_size)\n",
        "    if verbose:\n",
        "      print(f'Количество фрагментов: {len(fragments)}')\n",
        "\n",
        "    # Обрабатываем каждый фрагмент копирайтером\n",
        "    for fragment in tqdm(fragments):\n",
        "        processed_text = process_one_fragment(fragment)\n",
        "        result_text += f'{processed_text}\\n\\n'\n",
        "\n",
        "    # if verbose:\n",
        "    print('\\n\\nВесь текст обработан!')\n",
        "\n",
        "    return result_text\n",
        "\n",
        "\n",
        "# Функция очистки текста после копирайтера\n",
        "def text_cleaner(text, patterns_to_remove):\n",
        "    for pattern in patterns_to_remove:\n",
        "        text = re.sub(pattern, '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# Функция сохранения текста в файл\n",
        "def save_result(text, result_file_path=result_file_path, verbose=verbose):\n",
        "    with open(result_file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text)\n",
        "    # if verbose:\n",
        "    print(f'Результат сохранен в {result_file_path}!!!')\n",
        "\n",
        "\n",
        "# Получаем сырой результат работы копирайтера\n",
        "text = process_data(transcript.dict()['text'])\n",
        "\n",
        "# Чистим клинером\n",
        "patterns_to_remove = [\n",
        "        r' Раздел \\d+:',\n",
        "        r'\\+ 100% текста относящегося к этому разделу:',\n",
        "        r'\\(100% текста\\)',\n",
        "        r'Весь текст, относящийся к этому разделу:',\n",
        "        r'100% текста относящегося к этому разделу:',\n",
        "        r'100% текста относятся к этому разделу:',\n",
        "        r'Вот так можно разбить данный текст на разделы, сохраняя весь текст на 100%.',\n",
        "        r'Вот так можно разделить данный текст на разделы, сохраняя весь текст на 100%.',\n",
        "        r'Весь текст, относящийся к этому разделу:',\n",
        "        r'\\(100% текста относящегося к этому разделу\\)',\n",
        "        r'Разделы в тексте и их названия:',\n",
        "        r'Этот раздел содержит следующий текст:',\n",
        "        r'Текст, относящийся к этому разделу:',\n",
        "        r'Текст: ',\n",
        "        r'## Разделы и текст:',\n",
        "        r'Текст, относящийся к этому разделу:',\n",
        "        r'Текст, разбитый на разделы:',\n",
        "        r'Таким образом, текст разделен на соответствующие разделы, сохраняя весь текст на 100%.',\n",
        "        r'100% исходного текста относящегося к этому разделу:',\n",
        "        r'Название раздела: ',\n",
        "        r'Раздел: ',\n",
        "        r'По анализу текста, можно выделить следующие разделы:',\n",
        "        r'Надеюсь, это поможет вам разделить текст на соответствующие разделы:',\n",
        "        r'# Название раздела',\n",
        "        r'## Название подраздела + часть исходного текста',\n",
        "        r'## Название подраздела'\n",
        "    ]\n",
        "\n",
        "text = text_cleaner(text, patterns_to_remove)\n",
        "print(f'Результат:\\n\\n {text}')\n",
        "\n",
        "# Сохраняем результат в сессионном хранилище\n",
        "save_result(text)\n",
        "\n",
        "# # Разделяем текст на чанки (## разделы)\n",
        "# chunks = [chunk.strip() for chunk in text.split('##') if chunk.strip()]\n",
        "\n",
        "# # Создаем документы из чанков\n",
        "# docs = [Document(page_content=chunk, metadata={}) for chunk in chunks]\n",
        "\n",
        "# # Создаем индексную базу\n",
        "# db = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
        "\n",
        "# # Сохранение базы в сессионное хранилище\n",
        "# db.save_local('faiss_index')\n",
        "# print('База сохранена в сессионное хранилище (faiss_index)')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yommQkuivEFp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}